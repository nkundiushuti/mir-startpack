{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_End_to_end_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW3seEK2yWPd"
      },
      "source": [
        "End-to-end classification using deep learning on the waveform\n",
        "============================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cSJcgQwIrIq"
      },
      "source": [
        "## Installation of packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GugsmRYH_72"
      },
      "source": [
        "First of all we need to find out if the notebook is run on Colab and, if so, what version of cuda we have on the server. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iidt0GzvHHxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6568d5b9-d7ff-450a-c7f5-04565b2a42dc"
      },
      "source": [
        "# RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "# if RunningInCOLAB:\n",
        "#   !nvcc -V"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFs_ZDemz3Vf"
      },
      "source": [
        "Then we install the pytorch version for the corresponding cuda version and the other packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4ePrUY2yoCh",
        "outputId": "f4982ce9-5272-4166-fbed-5118504c9417"
      },
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip uninstall -y torchtext\n",
        "!pip install pytorch-lightning==1.1\n",
        "!pip install mirdata scikit-learn\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
            "Collecting torchaudio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Installing collected packages: torchaudio\n",
            "Successfully installed torchaudio-0.8.1\n",
            "Uninstalling torchtext-0.9.1:\n",
            "  Successfully uninstalled torchtext-0.9.1\n",
            "Collecting pytorch-lightning==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/d0/84a2f072cd407f93a1e50dff059656bce305f084e63a45cbbceb2fdb67b4/pytorch_lightning-1.1.0-py3-none-any.whl (665kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 7.0MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 14.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.1) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.1) (1.19.5)\n",
            "Collecting fsspec>=0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/d2/d05466997f7751a2c06a7a416b7d1f131d765f7916698d3fdcb3a4d037e5/fsspec-2021.6.0-py3-none-any.whl (114kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 27.3MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 21.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.1) (4.41.1)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.1) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->pytorch-lightning==1.1) (3.7.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (1.30.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (0.36.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (1.34.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (0.4.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.1) (57.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning==1.1) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.1) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.1) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.1) (3.1.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491070 sha256=c3ff428050592d5bec544494fd65bf0193bb1668735676c59ec8a78732319658\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: PyYAML, fsspec, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.4.1 fsspec-2021.6.0 future-0.18.2 pytorch-lightning-1.1.0\n",
            "Collecting mirdata\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/74/65b17d5ad5562e72000d034b0afd1f31aafcc9d0c5d3bc1e7980de36542f/mirdata-0.3.3-py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from mirdata) (3.0.4)\n",
            "Collecting pretty-midi>=0.2.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/8e/63c6e39a7a64623a9cd6aec530070c70827f6f8f40deec938f323d7b1e15/pretty_midi-0.2.9.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 44.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mirdata) (4.41.1)\n",
            "Requirement already satisfied: librosa>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from mirdata) (0.8.0)\n",
            "Collecting jams\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/1a/761a500a9512937931e325b8950195f17e5ba991c92d38d2079a2d8c5590/jams-0.3.4.tar.gz (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from mirdata) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from mirdata) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Collecting mido>=1.1.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/6d/e18a5b59ff086e1cd61d7fbf943d86c5f593a4e68bfc60215ab74210b22b/mido-1.2.10-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty-midi>=0.2.8->mirdata) (1.15.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata) (1.3.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata) (0.10.3.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata) (0.51.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata) (0.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.8.0->mirdata) (2.1.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from jams->mirdata) (1.1.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from jams->mirdata) (2.4.0)\n",
            "Collecting jsonschema>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/8f/51e89ce52a085483359217bc72cdbf6e75ee595d5b1d4b5ade40c7e018b8/jsonschema-3.2.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.7MB/s \n",
            "\u001b[?25hCollecting mir_eval>=0.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/fe/be4f7a59ed71938e21e89f23afe93eea0d39eb3e77f83754a12028cf1a68/mir_eval-0.6.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->mirdata) (1.24.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->mirdata) (20.9)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.8.0->mirdata) (1.4.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa>=0.8.0->mirdata) (1.14.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.8.0->mirdata) (57.0.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.8.0->mirdata) (0.34.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->jams->mirdata) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->jams->mirdata) (2018.9)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.0->jams->mirdata) (0.17.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.0->jams->mirdata) (21.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.0->jams->mirdata) (4.0.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval>=0.5->jams->mirdata) (0.18.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa>=0.8.0->mirdata) (2.4.7)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa>=0.8.0->mirdata) (2.20)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.0->jams->mirdata) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.0->jams->mirdata) (3.4.1)\n",
            "Building wheels for collected packages: pretty-midi, jams, mir-eval\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-cp37-none-any.whl size=5591958 sha256=c9820c6cac585aae045b681e04be8830a060ba75e1562fe5c2b0dcfa2706acc2\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a1/c6/b5697841db1112c6e5866d75a6b6bf1bef73b874782556ba66\n",
            "  Building wheel for jams (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jams: filename=jams-0.3.4-cp37-none-any.whl size=64923 sha256=79892c6d4ec9c18fd5ee06250f8bc82c30fa792e06da74acbaff42dc8bcf229f\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/b8/c7/092096a3af0eb864cca4e79a44bc883dee5b9767da2c6f8ab3\n",
            "  Building wheel for mir-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.6-cp37-none-any.whl size=96515 sha256=465293940a749138786a3322d3d62f5db6bf4ea089dc33b70d9b9a426fd4de18\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/ce/30/730fa72addf275e49d90683b01b3613048b4be3bf7ff8eb6ec\n",
            "Successfully built pretty-midi jams mir-eval\n",
            "\u001b[31mERROR: nbclient 0.5.3 has requirement jupyter-client>=6.1.5, but you'll have jupyter-client 5.3.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mido, pretty-midi, jsonschema, mir-eval, jams, mirdata\n",
            "  Found existing installation: jsonschema 2.6.0\n",
            "    Uninstalling jsonschema-2.6.0:\n",
            "      Successfully uninstalled jsonschema-2.6.0\n",
            "Successfully installed jams-0.3.4 jsonschema-3.2.0 mido-1.2.10 mir-eval-0.6 mirdata-0.3.3 pretty-midi-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBrIMBt6Jm34"
      },
      "source": [
        "We import the packages and we set the random_seed for our experiments. The random seed makes sure the experiment is reproducible on this environment.\n",
        "\n",
        "We use mirdata to load the datasets, sklearn for data partitioning, torchaudio to load and transform audio files, and pytorch lightning on top of pytorch for machine learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK5QL1yMHFmx",
        "outputId": "6e00325b-ba9b-4a05-e602-3f14b430b672"
      },
      "source": [
        "import mirdata\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import pytorch_lightning as pl\n",
        "random_seed=0\n",
        "pl.utilities.seed.seed_everything(seed=random_seed)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pokcYI_tK2rO"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkgW6tvrKeaL"
      },
      "source": [
        "We initialize Mridangam stroke a collection of 7162 audio examples of individual strokes of the Mridangam in various tonics. The dataset comprises of 10 different strokes played on Mridangams with 6 different tonic values. \n",
        "\n",
        "In this experiment we predict 10 stroke classes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y6vQl5UHOkV"
      },
      "source": [
        "mridangam = mirdata.initialize(\"mridangam_stroke\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvCFJiXFKhYk"
      },
      "source": [
        "First time the dataset needs to be downloaded. This is fairly easy with the public datasets in mirdata, by calling the download method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSG57fP2gqDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8723d44-e0b7-4a93-9cc4-6a383f6adc8a"
      },
      "source": [
        "mridangam.download()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Downloading ['remote_data'] to /root/mir_datasets/mridangam_stroke\n",
            "INFO: [remote_data] downloading mridangam_stroke_1.5.zip\n",
            "124MB [00:13, 9.71MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yurv0W8K5nP",
        "outputId": "7a9afdbc-e703-454f-8ccf-e3e3c8d43939"
      },
      "source": [
        "import IPython.display as ipd\n",
        "mridangam.validate()  # validate dataset\n",
        "track = mridangam.choice_track()  # load a random track\n",
        "x, sr = track.audio\n",
        "ipd.Audio(track.audio_path)\n",
        "print(track)  # see what data a track contains"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6976/6976 [00:00<00:00, 9264.04it/s]\n",
            "INFO: Success: the dataset is complete and all files are valid.\n",
            "INFO: --------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Track(\n",
            "  audio_path=\"/root/mir_datasets/mridangam_stroke/mridangam_stroke_1.5/E/231180__akshaylaya__thom-e-077.wav\",\n",
            "  stroke_name=\"thom\",\n",
            "  tonic=\"E\",\n",
            "  track_id=\"231180\",\n",
            "  audio: The track's audio\n",
            "\n",
            "        Returns,\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzWTTqxmKtwE"
      },
      "source": [
        "\n",
        "In order to use this dataset with pytorch, we extend the Dataset object to load the audio and annotations in our dataset, according to these [instructions](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "We basically need to write three methods:\n",
        "\n",
        "\n",
        "*   __init__\n",
        "*   __len__\n",
        "*   __getitem__ to return each pair of audio array and class label\n",
        "\n",
        "\n",
        "This is how a prototype of this class could look like:\n",
        "\n",
        "```\n",
        "class MridangamDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "      self.track_ids = dataset.track_ids\n",
        "    def __getitem__(self, index):\n",
        "      # load data\n",
        "      audio = load_audio(self.track_ids[index])\n",
        "      label = self.track_ids[index].label\n",
        "      # split audio in a fixed size array\n",
        "      audio = audio[:seq_duration] \n",
        "      return audio,label\n",
        "    def __len__(self):\n",
        "      return len(self.tracks_ids)\n",
        "\n",
        "```\n",
        "\n",
        "Let's implement the class:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irATuWrpHblx"
      },
      "source": [
        "class MridangamDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mirdataset,\n",
        "        seq_duration=0.5,\n",
        "        random_start=True,\n",
        "        resample=8000,\n",
        "        subset=0,\n",
        "        train_split=0.8,\n",
        "        test_split=0.2,\n",
        "        random_seed=42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.seq_duration = seq_duration\n",
        "        self.dataset = mirdataset\n",
        "        self.track_ids = self.dataset.track_ids\n",
        "        self.tracks = self.dataset.load_tracks()\n",
        "        self.resample = resample\n",
        "        self.set = subset\n",
        "        self.random_start = random_start\n",
        "\n",
        "        #### build a list with labels \n",
        "        labels = [self.dataset.track(i).stroke_name for i in self.track_ids]\n",
        "        unique_labels = list(set(labels)) ### unique labels\n",
        "        self.labels = {label:i for i,label in enumerate(unique_labels)}\n",
        "\n",
        "        #### build the three subsets: train, validation, test using train_test_split, a stratified split with the labels\n",
        "        self.trackids_train, self.trackids_test = sklearn.model_selection.train_test_split(self.track_ids, train_size=1-test_split, random_state=random_seed, stratify=labels)\n",
        "        train_labels = [l for l,i in zip(labels,self.track_ids) if i in self.trackids_train]\n",
        "        self.trackids_train, self.trackids_valid = sklearn.model_selection.train_test_split(self.trackids_train, train_size=train_split, random_state=random_seed, stratify=train_labels)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        #### get the file with index in the corresponding subset\n",
        "        if self.set==0:\n",
        "            track_id = self.trackids_train[index]\n",
        "        elif self.set==1:\n",
        "            track_id = self.trackids_valid[index]\n",
        "        elif self.set==2:\n",
        "            track_id = self.trackids_test[index]\n",
        "        track = self.dataset.track(track_id)\n",
        "\n",
        "        #### compute start and end frames to read from the disk\n",
        "        # si, ei = torchaudio.info(track.audio_path)\n",
        "        # sample_rate, channels, length = si.rate, si.channels, si.length\n",
        "        ####alternative\n",
        "        metadata = torchaudio.info(track.audio_path)\n",
        "        sample_rate = metadata.sample_rate \n",
        "        channels = metadata.num_channels\n",
        "        length = metadata.num_frames\n",
        "        duration = length / sample_rate\n",
        "\n",
        "        \n",
        "        offset = 0\n",
        "        if self.seq_duration>duration:\n",
        "            num_frames = length\n",
        "        else:\n",
        "            num_frames = int(np.floor(self.seq_duration * sample_rate))\n",
        "\n",
        "\n",
        "        #### get audio frames corresponding to offset and num_frames from the disk\n",
        "        audio_signal, sample_rate = torchaudio.load(filepath=track.audio_path, frame_offset=offset,num_frames=num_frames)\n",
        "        #### alternative\n",
        "        #audio_signal, sample_rate = torchaudio.load(filepath=track.audio_path, offset=offset,num_frames=num_frames)\n",
        "\n",
        "        #### zero pad if the size is smaller than seq_duration\n",
        "        seq_duration_samples = int(self.seq_duration * sample_rate)\n",
        "        total_samples = audio_signal.shape[-1]\n",
        "        if seq_duration_samples>total_samples:\n",
        "            audio_signal = torch.nn.ConstantPad2d((0,seq_duration_samples-total_samples,0,0),0)(audio_signal)\n",
        "\n",
        "        #### resample\n",
        "        audio_signal = torchaudio.transforms.Resample(sample_rate, self.resample)(audio_signal)\n",
        "\n",
        "        return audio_signal, self.labels[track.stroke_name] \n",
        "\n",
        "    def __len__(self):\n",
        "        if self.set==0:\n",
        "            return len(self.trackids_train)\n",
        "        elif self.set==1:\n",
        "            return len(self.trackids_valid)\n",
        "        else:\n",
        "            return len(self.trackids_test)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM4Wk3tsRnqg"
      },
      "source": [
        "We initialize the dataset objects for train, validation, and test. We define the corresponding pytorch objects for data loading, defining the batch_size (paralellization on the GPU) and the num_workers ( data loading paralellization on CPU/memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072HyKNfHs7q"
      },
      "source": [
        "#### Pytorch dataset loaders\n",
        "train_dataset = MridangamDataset(mirdataset=mridangam,subset=0, random_seed=random_seed)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64,num_workers=2,pin_memory=True)\n",
        "valid_dataset = MridangamDataset(mirdataset=mridangam,subset=1, random_seed=random_seed)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=64,num_workers=2,pin_memory=True)\n",
        "test_dataset = MridangamDataset(mirdataset=mridangam,subset=2, random_seed=random_seed)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64,num_workers=2,pin_memory=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRowsZK3HfDi"
      },
      "source": [
        "\n",
        "**Which batch size/learning rate?**\n",
        "\n",
        "Theory suggests that when multiplying the batch size by k, one should multiply the learning rate by sqrt(k) to keep the variance in the gradient expectation constant. See page 5 at A. Krizhevsky. One weird trick for parallelizing convolutional neural networks: https://arxiv.org/abs/1404.5997\n",
        "\n",
        "However, recent experiments with large mini-batches suggest for a simpler linear scaling rule, i.e multiply your learning rate by k when using mini-batch size of kN. See P.Goyal et al.: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour https://arxiv.org/abs/1706.02677"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keh79hDpSUHG"
      },
      "source": [
        "## Training a pytorch lightning classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwMhNSUSrRJ"
      },
      "source": [
        "We extend the pytorch lightning module according to the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/new-project.html). This may contain a definition of the layers in the neural network and how the data flows (how the layers are connected). You may overwrite other functions from `pl.LightningModule`, as described [here](https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html). The most important are `training_step` and `configure_optimizers`, in which we define the training loss and the optimizers.\n",
        "\n",
        "W = W - lr * Delta(W) -> Stochastic gradient descent\n",
        "W = [w1 ... w10] [l1...l10] \n",
        "\n",
        "```\n",
        ">>> class LitModel(pl.LightningModule):\n",
        "...\n",
        "...     def __init__(self):\n",
        "...         super().__init__()\n",
        "...         self.l1 = torch.nn.Linear(28 * 28, 10)\n",
        "...\n",
        "...     def forward(self, x):\n",
        "...         return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
        "...\n",
        "...     def training_step(self, batch, batch_idx):\n",
        "...         x, y = batch\n",
        "...         y_hat = self.forward(x)\n",
        "...         loss = F.cross_entropy(y_hat, y)\n",
        "...         return loss\n",
        "...\n",
        "...     def configure_optimizers(self):\n",
        "...         return torch.optim.Adam(self.parameters(), lr=0.02)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRF81_4YZ4cf"
      },
      "source": [
        "To predict the 10 classes of the Mridangam stroke dataset on the raw audio files, we train a version of the M5 neural network which has been used in speech command recognition using waveforms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgBov7QIH3a8"
      },
      "source": [
        "class M5(pl.LightningModule):\n",
        "    '''\n",
        "    M5 neural net taken from: https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html\n",
        "    '''\n",
        "    def __init__(self, n_input=1, n_output=10, stride=8, n_channel=32):\n",
        "        super().__init__()\n",
        "        #### network\n",
        "        self.conv1 = torch.nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = torch.nn.MaxPool1d(4)\n",
        "        self.conv2 = torch.nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = torch.nn.MaxPool1d(4)\n",
        "        self.conv3 = torch.nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = torch.nn.MaxPool1d(4)\n",
        "        self.conv4 = torch.nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = torch.nn.MaxPool1d(4)\n",
        "        self.fc1 = torch.nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "        #### metrics\n",
        "        self.train_acc = pl.metrics.Accuracy()\n",
        "        self.valid_acc = pl.metrics.Accuracy()\n",
        "        self.test_acc = pl.metrics.Accuracy()\n",
        "        self.test_cm = pl.metrics.classification.ConfusionMatrix(num_classes=n_output)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.nn.functional.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.nn.functional.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.nn.functional.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.nn.functional.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        # x = torch.nn.functional.avg_pool1d(x) #, kernel_size=x.shape[-1],stride=1\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return torch.nn.functional.log_softmax(x, dim=2).squeeze(1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        waveform, label = batch\n",
        "        output = self.forward(waveform)\n",
        "        ### why log softmax and nll loss: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
        "        loss = torch.nn.functional.nll_loss(output, label)\n",
        "        self.log('train_loss', loss)\n",
        "        self.train_acc(output, label)\n",
        "        self.log('train_acc', self.train_acc, on_step=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        waveform, label = batch\n",
        "        output = self.forward(waveform)\n",
        "        loss = torch.nn.functional.nll_loss(output, label)\n",
        "        self.log('val_loss', loss)\n",
        "        self.valid_acc(output, label)\n",
        "        self.log('valid_acc', self.valid_acc, on_step=True, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        waveform, label = batch\n",
        "        output = self.forward(waveform)\n",
        "        loss = torch.nn.functional.nll_loss(output, label)\n",
        "        self.log('test_loss', loss)\n",
        "        self.test_acc(output, label)\n",
        "        self.log('test_acc', self.test_acc, on_step=True, on_epoch=True)\n",
        "        self.test_cm(output, label)\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        # log epoch metric\n",
        "        self.log('train_acc', self.train_acc.compute(), prog_bar=True)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.log('val_acc', self.valid_acc.compute(), prog_bar=True)\n",
        "\n",
        "    def get_progress_bar_dict(self):\n",
        "        # don't show the version number\n",
        "        items = super().get_progress_bar_dict()\n",
        "        items.pop(\"v_num\", None)\n",
        "        return items\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-2,weight_decay=0.0001)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # reduce the learning after 10 epochs by a factor of 10\n",
        "        return [optimizer], [scheduler]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBgC2yQhZdFx"
      },
      "source": [
        "We train the model defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzLG38D1H9XQ"
      },
      "source": [
        "#### Initialize the model\n",
        "model = M5(n_input=train_dataset[0][0].shape[0], n_output=len(train_dataset.labels))\n",
        "\n",
        "#### Initialize a trainer\n",
        "#trainer = pl.Trainer(gpus=1, max_epochs=3, progress_bar_refresh_rate=10)\n",
        "\n",
        "#### Train the model\n",
        "#trainer.fit(model, train_loader, valid_loader)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIg6fmr4Zj4c"
      },
      "source": [
        "Once the model is trained we can use it to process data, save it, get the metrics on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l9QTVHAIJcx"
      },
      "source": [
        "# #### Put the model in production\n",
        "# model.eval()\n",
        "\n",
        "# #### Compute metrics on the test set\n",
        "# trainer.test(test_dataloaders=test_loader)\n",
        "\n",
        "# #### Compute confusion matrix on the test set\n",
        "# confusion_matrix = model.test_cm.compute().cpu().numpy()\n",
        "# import matplotlib.pyplot as plt\n",
        "# fig, ax = plt.subplots()\n",
        "# ax.matshow(confusion_matrix)\n",
        "# ax.set_xticks(range(len(train_dataset.labels)))\n",
        "# ax.set_yticks(range(len(train_dataset.labels)))\n",
        "# ax.set_xticklabels(train_dataset.labels)\n",
        "# ax.set_yticklabels(train_dataset.labels)\n",
        "# plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}