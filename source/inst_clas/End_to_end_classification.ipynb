{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "End_to_end_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW3seEK2yWPd"
      },
      "source": [
        "End-to-end classification using deep learning on the waveform\n",
        "============================================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cSJcgQwIrIq"
      },
      "source": [
        "## Installation of packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GugsmRYH_72"
      },
      "source": [
        "First of all we need to find out if the notebook is run on Colab and, if so, what version of cuda we have on the server. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iidt0GzvHHxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a46114-6136-48e2-afa3-1befd2483629"
      },
      "source": [
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "if RunningInCOLAB:\n",
        "  !nvcc -V"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFs_ZDemz3Vf"
      },
      "source": [
        "Then we install the pytorch version for the corresponding cuda version and the other packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4ePrUY2yoCh"
      },
      "source": [
        "%%capture\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip uninstall -y torchtext\n",
        "!pip install pytorch-lightning==1.1\n",
        "!pip install mirdata scikit-learn\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBrIMBt6Jm34"
      },
      "source": [
        "We import the packages and we set the random_seed for our experiments. The random seed makes sure the experiment is reproducible on this environment.\n",
        "\n",
        "We use mirdata to load the datasets, sklearn for data partitioning, torchaudio to load and transform audio files, and pytorch lightning on top of pytorch for machine learning. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK5QL1yMHFmx",
        "outputId": "0146473a-80f0-4949-f543-08035a1d02c1"
      },
      "source": [
        "import mirdata\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import random\n",
        "import torch\n",
        "import torchaudio\n",
        "import pytorch_lightning as pl\n",
        "random_seed=0\n",
        "pl.utilities.seed.seed_everything(seed=random_seed)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pokcYI_tK2rO"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkgW6tvrKeaL"
      },
      "source": [
        "We initialize Mridangam stroke a collection of 7162 audio examples of individual strokes of the Mridangam in various tonics. The dataset comprises of 10 different strokes played on Mridangams with 6 different tonic values. \n",
        "\n",
        "In this experiment we predict 10 stroke classes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y6vQl5UHOkV"
      },
      "source": [
        "mridangam = mirdata.initialize(\"mridangam_stroke\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvCFJiXFKhYk"
      },
      "source": [
        "First time the dataset needs to be downloaded. This is fairly easy with the public datasets in mirdata, by calling the download method. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSG57fP2gqDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391cdf93-bbe8-46de-e1aa-c0086c95e979"
      },
      "source": [
        "mridangam.download()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: Downloading ['remote_data'] to /root/mir_datasets/mridangam_stroke\n",
            "INFO: [remote_data] downloading mridangam_stroke_1.5.zip\n",
            "124MB [00:06, 20.4MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yurv0W8K5nP",
        "outputId": "c2d28bdd-2fb1-4510-da0b-7f2d445eac3b"
      },
      "source": [
        "import IPython.display as ipd\n",
        "mridangam.validate()  # validate dataset\n",
        "track = mridangam.choice_track()  # load a random track\n",
        "x, sr = track.audio\n",
        "ipd.Audio(track.audio_path)\n",
        "print(track)  # see what data a track contains"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 6976/6976 [00:00<00:00, 10858.98it/s]\n",
            "INFO: Success: the dataset is complete and all files are valid.\n",
            "INFO: --------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Track(\n",
            "  audio_path=\"/root/mir_datasets/mridangam_stroke/mridangam_stroke_1.5/E/231180__akshaylaya__thom-e-077.wav\",\n",
            "  stroke_name=\"thom\",\n",
            "  tonic=\"E\",\n",
            "  track_id=\"231180\",\n",
            "  audio: The track's audio\n",
            "\n",
            "        Returns,\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzWTTqxmKtwE"
      },
      "source": [
        "\n",
        "In order to use this dataset with pytorch, we extend the Dataset object to load the audio and annotations in our dataset, according to these [instructions](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "We basically need to write three methods:\n",
        "\n",
        "\n",
        "*   __init__\n",
        "*   __len__\n",
        "*   __getitem__ to return each pair of audio array and class label\n",
        "\n",
        "\n",
        "This is how a prototype of this class could look like:\n",
        "\n",
        "```\n",
        "class MridangamDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self):\n",
        "      self.track_ids = dataset.track_ids\n",
        "    def __getitem__(self, index):\n",
        "      # load data\n",
        "      audio = load_audio(self.track_ids[index])\n",
        "      label = self.track_ids[index].label\n",
        "      # split audio in a fixed size array\n",
        "      audio = audio[:seq_duration] \n",
        "      return audio,label\n",
        "    def __len__(self):\n",
        "      return len(self.tracks_ids)\n",
        "\n",
        "```\n",
        "\n",
        "Let's implement the class:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irATuWrpHblx"
      },
      "source": [
        "class MridangamDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        mirdataset,\n",
        "        seq_duration=0.5,\n",
        "        random_start=True,\n",
        "        resample=8000,\n",
        "        subset=0,\n",
        "        train_split=0.8,\n",
        "        test_split=0.2,\n",
        "        random_seed=42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.seq_duration = seq_duration\n",
        "        self.dataset = mirdataset\n",
        "        self.track_ids = self.dataset.track_ids\n",
        "        self.tracks = self.dataset.load_tracks()\n",
        "        self.resample = resample\n",
        "        self.set = subset\n",
        "        self.random_start = random_start\n",
        "\n",
        "        #### build a list with labels \n",
        "        labels = [self.dataset.track(i).stroke_name for i in self.track_ids]\n",
        "        unique_labels = list(set(labels)) ### unique labels\n",
        "        self.labels = {label:i for i,label in enumerate(unique_labels)}\n",
        "\n",
        "        #### build the three subsets: train, validation, test using train_test_split, a stratified split with the labels\n",
        "        self.trackids_train, self.trackids_test = sklearn.model_selection.train_test_split(self.track_ids, train_size=1-test_split, random_state=random_seed, stratify=labels)\n",
        "        train_labels = [l for l,i in zip(labels,self.track_ids) if i in self.trackids_train]\n",
        "        self.trackids_train, self.trackids_valid = sklearn.model_selection.train_test_split(self.trackids_train, train_size=train_split, random_state=random_seed, stratify=train_labels)\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        #### get the file with index in the corresponding subset\n",
        "        if self.set==0:\n",
        "            track_id = self.trackids_train[index]\n",
        "        elif self.set==1:\n",
        "            track_id = self.trackids_valid[index]\n",
        "        elif self.set==2:\n",
        "            track_id = self.trackids_test[index]\n",
        "        track = self.dataset.track(track_id)\n",
        "\n",
        "        #### compute start and end frames to read from the disk\n",
        "        # si, ei = torchaudio.info(track.audio_path)\n",
        "        # sample_rate, channels, length = si.rate, si.channels, si.length\n",
        "        ####alternative\n",
        "        metadata = torchaudio.info(track.audio_path)\n",
        "        sample_rate = metadata.sample_rate \n",
        "        channels = metadata.num_channels\n",
        "        length = metadata.num_frames\n",
        "        duration = length / sample_rate\n",
        "\n",
        "        \n",
        "        offset = 0\n",
        "        if self.seq_duration>duration:\n",
        "            num_frames = length\n",
        "        else:\n",
        "            num_frames = int(np.floor(self.seq_duration * sample_rate))\n",
        "\n",
        "\n",
        "        #### get audio frames corresponding to offset and num_frames from the disk\n",
        "        audio_signal, sample_rate = torchaudio.load(filepath=track.audio_path, frame_offset=offset,num_frames=num_frames)\n",
        "        #### alternative\n",
        "        #audio_signal, sample_rate = torchaudio.load(filepath=track.audio_path, offset=offset,num_frames=num_frames)\n",
        "\n",
        "        #### zero pad if the size is smaller than seq_duration\n",
        "        seq_duration_samples = int(self.seq_duration * sample_rate)\n",
        "        total_samples = audio_signal.shape[-1]\n",
        "        if seq_duration_samples>total_samples:\n",
        "            audio_signal = torch.nn.ConstantPad2d((0,seq_duration_samples-total_samples,0,0),0)(audio_signal)\n",
        "\n",
        "        #### resample\n",
        "        audio_signal = torchaudio.transforms.Resample(sample_rate, self.resample)(audio_signal)\n",
        "\n",
        "        return audio_signal, self.labels[track.stroke_name] \n",
        "\n",
        "    def __len__(self):\n",
        "        if self.set==0:\n",
        "            return len(self.trackids_train)\n",
        "        elif self.set==1:\n",
        "            return len(self.trackids_valid)\n",
        "        else:\n",
        "            return len(self.trackids_test)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM4Wk3tsRnqg"
      },
      "source": [
        "We initialize the dataset objects for train, validation, and test. We define the corresponding pytorch objects for data loading, defining the batch_size (paralellization on the GPU) and the num_workers ( data loading paralellization on CPU/memory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "072HyKNfHs7q"
      },
      "source": [
        "#### Pytorch dataset loaders\n",
        "train_dataset = MridangamDataset(mirdataset=mridangam,subset=0, random_seed=random_seed)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=64,num_workers=2,pin_memory=True)\n",
        "valid_dataset = MridangamDataset(mirdataset=mridangam,subset=1, random_seed=random_seed)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=64,num_workers=2,pin_memory=True)\n",
        "test_dataset = MridangamDataset(mirdataset=mridangam,subset=2, random_seed=random_seed)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=64,num_workers=2,pin_memory=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRowsZK3HfDi"
      },
      "source": [
        "\n",
        "**Which batch size/learning rate?**\n",
        "\n",
        "Theory suggests that when multiplying the batch size by k, one should multiply the learning rate by sqrt(k) to keep the variance in the gradient expectation constant. See page 5 at A. Krizhevsky. One weird trick for parallelizing convolutional neural networks: https://arxiv.org/abs/1404.5997\n",
        "\n",
        "However, recent experiments with large mini-batches suggest for a simpler linear scaling rule, i.e multiply your learning rate by k when using mini-batch size of kN. See P.Goyal et al.: Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour https://arxiv.org/abs/1706.02677"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keh79hDpSUHG"
      },
      "source": [
        "## Training a pytorch lightning classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpwMhNSUSrRJ"
      },
      "source": [
        "We extend the pytorch lightning module according to the [documentation](https://pytorch-lightning.readthedocs.io/en/stable/new-project.html). This may contain a definition of the layers in the neural network and how the data flows (how the layers are connected). You may overwrite other functions from `pl.LightningModule`, as described [here](https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html). The most important are `training_step` and `configure_optimizers`, in which we define the training loss and the optimizers.\n",
        "\n",
        "W = W - lr * Delta(W) -> Stochastic gradient descent\n",
        "W = [w1 ... w10] [l1...l10] \n",
        "\n",
        "```\n",
        ">>> class LitModel(pl.LightningModule):\n",
        "...\n",
        "...     def __init__(self):\n",
        "...         super().__init__()\n",
        "...         self.l1 = torch.nn.Linear(28 * 28, 10)\n",
        "...\n",
        "...     def forward(self, x):\n",
        "...         return torch.relu(self.l1(x.view(x.size(0), -1)))\n",
        "...\n",
        "...     def training_step(self, batch, batch_idx):\n",
        "...         x, y = batch\n",
        "...         y_hat = self.forward(x)\n",
        "...         loss = F.cross_entropy(y_hat, y)\n",
        "...         return loss\n",
        "...\n",
        "...     def configure_optimizers(self):\n",
        "...         return torch.optim.Adam(self.parameters(), lr=0.02)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRF81_4YZ4cf"
      },
      "source": [
        "To predict the 10 classes of the Mridangam stroke dataset on the raw audio files, we train a version of the M5 neural network which has been used in speech command recognition using waveforms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgBov7QIH3a8"
      },
      "source": [
        "class M5(pl.LightningModule):\n",
        "    '''\n",
        "    M5 neural net taken from: https://pytorch.org/tutorials/intermediate/speech_command_recognition_with_torchaudio.html\n",
        "    '''\n",
        "    def __init__(self, n_input=1, n_output=10, stride=8, n_channel=32):\n",
        "        super().__init__()\n",
        "        #### network\n",
        "        self.conv1 = torch.nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(n_channel)\n",
        "        self.pool1 = torch.nn.MaxPool1d(4)\n",
        "        self.conv2 = torch.nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(n_channel)\n",
        "        self.pool2 = torch.nn.MaxPool1d(4)\n",
        "        self.conv3 = torch.nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool3 = torch.nn.MaxPool1d(4)\n",
        "        self.conv4 = torch.nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(2 * n_channel)\n",
        "        self.pool4 = torch.nn.MaxPool1d(4)\n",
        "        self.fc1 = torch.nn.Linear(2 * n_channel, n_output)\n",
        "\n",
        "        #### metrics\n",
        "        self.train_acc = pl.metrics.Accuracy()\n",
        "        self.valid_acc = pl.metrics.Accuracy()\n",
        "        self.test_acc = pl.metrics.Accuracy()\n",
        "        self.test_cm = pl.metrics.classification.ConfusionMatrix(num_classes=n_output)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = torch.nn.functional.relu(self.bn1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = torch.nn.functional.relu(self.bn2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.nn.functional.relu(self.bn3(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.nn.functional.relu(self.bn4(x))\n",
        "        x = self.pool4(x)\n",
        "        # x = torch.nn.functional.avg_pool1d(x) #, kernel_size=x.shape[-1],stride=1\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.fc1(x)\n",
        "        return torch.nn.functional.log_softmax(x, dim=2).squeeze(1)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        waveform, label = batch\n",
        "        output = self.forward(waveform)\n",
        "        ### why log softmax and nll loss: https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
        "        loss = torch.nn.functional.nll_loss(output, label)\n",
        "        self.log('train_loss', loss)\n",
        "        self.train_acc(output, label)\n",
        "        self.log('train_acc', self.train_acc, on_step=True, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        waveform, label = batch\n",
        "        output = self.forward(waveform)\n",
        "        loss = torch.nn.functional.nll_loss(output, label)\n",
        "        self.log('val_loss', loss)\n",
        "        self.valid_acc(output, label)\n",
        "        self.log('valid_acc', self.valid_acc, on_step=True, on_epoch=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        waveform, label = batch\n",
        "        output = self.forward(waveform)\n",
        "        loss = torch.nn.functional.nll_loss(output, label)\n",
        "        self.log('test_loss', loss)\n",
        "        self.test_acc(output, label)\n",
        "        self.log('test_acc', self.test_acc, on_step=True, on_epoch=True)\n",
        "        self.test_cm(output, label)\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        # log epoch metric\n",
        "        self.log('train_acc', self.train_acc.compute(), prog_bar=True)\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        self.log('val_acc', self.valid_acc.compute(), prog_bar=True)\n",
        "\n",
        "    def get_progress_bar_dict(self):\n",
        "        # don't show the version number\n",
        "        items = super().get_progress_bar_dict()\n",
        "        items.pop(\"v_num\", None)\n",
        "        return items\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=2e-2,weight_decay=0.0001)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)  # reduce the learning after 10 epochs by a factor of 10\n",
        "        return [optimizer], [scheduler]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBgC2yQhZdFx"
      },
      "source": [
        "We train the model defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzLG38D1H9XQ",
        "outputId": "704d7d5c-d39f-405f-d1eb-51e041d57761"
      },
      "source": [
        "%%capture\n",
        "#### Initialize the model\n",
        "model = M5(n_input=train_dataset[0][0].shape[0], n_output=len(train_dataset.labels))\n",
        "\n",
        "#### Initialize a trainer\n",
        "trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=10)\n",
        "\n",
        "#### Train the model\n",
        "trainer.fit(model, train_loader, valid_loader)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "INFO: GPU available: True, used: True\n",
            "TPU available: None, using: 0 TPU cores\n",
            "INFO: TPU available: None, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "   | Name      | Type            | Params\n",
            "-----------------------------------------------\n",
            "0  | conv1     | Conv1d          | 2.6 K \n",
            "1  | bn1       | BatchNorm1d     | 64    \n",
            "2  | pool1     | MaxPool1d       | 0     \n",
            "3  | conv2     | Conv1d          | 3.1 K \n",
            "4  | bn2       | BatchNorm1d     | 64    \n",
            "5  | pool2     | MaxPool1d       | 0     \n",
            "6  | conv3     | Conv1d          | 6.2 K \n",
            "7  | bn3       | BatchNorm1d     | 128   \n",
            "8  | pool3     | MaxPool1d       | 0     \n",
            "9  | conv4     | Conv1d          | 12.4 K\n",
            "10 | bn4       | BatchNorm1d     | 128   \n",
            "11 | pool4     | MaxPool1d       | 0     \n",
            "12 | fc1       | Linear          | 650   \n",
            "13 | train_acc | Accuracy        | 0     \n",
            "14 | valid_acc | Accuracy        | 0     \n",
            "15 | test_acc  | Accuracy        | 0     \n",
            "16 | test_cm   | ConfusionMatrix | 0     \n",
            "-----------------------------------------------\n",
            "25.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "25.3 K    Total params\n",
            "INFO: \n",
            "   | Name      | Type            | Params\n",
            "-----------------------------------------------\n",
            "0  | conv1     | Conv1d          | 2.6 K \n",
            "1  | bn1       | BatchNorm1d     | 64    \n",
            "2  | pool1     | MaxPool1d       | 0     \n",
            "3  | conv2     | Conv1d          | 3.1 K \n",
            "4  | bn2       | BatchNorm1d     | 64    \n",
            "5  | pool2     | MaxPool1d       | 0     \n",
            "6  | conv3     | Conv1d          | 6.2 K \n",
            "7  | bn3       | BatchNorm1d     | 128   \n",
            "8  | pool3     | MaxPool1d       | 0     \n",
            "9  | conv4     | Conv1d          | 12.4 K\n",
            "10 | bn4       | BatchNorm1d     | 128   \n",
            "11 | pool4     | MaxPool1d       | 0     \n",
            "12 | fc1       | Linear          | 650   \n",
            "13 | train_acc | Accuracy        | 0     \n",
            "14 | valid_acc | Accuracy        | 0     \n",
            "15 | test_acc  | Accuracy        | 0     \n",
            "16 | test_cm   | ConfusionMatrix | 0     \n",
            "-----------------------------------------------\n",
            "25.3 K    Trainable params\n",
            "0         Non-trainable params\n",
            "25.3 K    Total params\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIg6fmr4Zj4c"
      },
      "source": [
        "Once the model is trained we can use it to process data, save it, get the metrics on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRfkS5lmj_4T"
      },
      "source": [
        "%%capture\n",
        "#### Put the model in production\n",
        "model.eval()\n",
        "\n",
        "#### Compute metrics on the test set\n",
        "trainer.test(test_dataloaders=test_loader)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "4l9QTVHAIJcx",
        "outputId": "2ba5ebf0-22b3-4471-8f7b-c0cd7a65218e"
      },
      "source": [
        "#### Compute confusion matrix on the test set\n",
        "confusion_matrix = model.test_cm.compute().cpu().numpy()\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "ax.matshow(confusion_matrix)\n",
        "ax.set_xticks(range(len(train_dataset.labels)))\n",
        "ax.set_yticks(range(len(train_dataset.labels)))\n",
        "ax.set_xticklabels(train_dataset.labels)\n",
        "ax.set_yticklabels(train_dataset.labels)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}